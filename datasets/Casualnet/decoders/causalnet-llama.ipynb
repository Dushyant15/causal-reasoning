{"cells":[{"cell_type":"code","execution_count":null,"id":"bd877828","metadata":{"execution":{"iopub.execute_input":"2024-02-11T18:16:58.123988Z","iopub.status.busy":"2024-02-11T18:16:58.123686Z","iopub.status.idle":"2024-02-11T18:16:58.127903Z","shell.execute_reply":"2024-02-11T18:16:58.127189Z"},"papermill":{"duration":0.010584,"end_time":"2024-02-11T18:16:58.129809","exception":false,"start_time":"2024-02-11T18:16:58.119225","status":"completed"},"tags":[],"id":"bd877828"},"outputs":[],"source":["!pip install -q accelerate==0.21.0 bitsandbytes==0.40.2\n","# !pip install -q transformers==4.30.0\n","# !pip install -q git+https://github.com/NousResearch/Llama.git"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7VRydPqnAMCK","executionInfo":{"status":"ok","timestamp":1707954513384,"user_tz":480,"elapsed":972,"user":{"displayName":"Kshiteesh Hegde","userId":"00079137205506997041"}},"outputId":"595aeb97-f041-4aa5-bb7f-51a41dcd29c1"},"id":"7VRydPqnAMCK","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":null,"id":"edeb2490","metadata":{"execution":{"iopub.execute_input":"2024-02-11T00:18:56.336523Z","iopub.status.busy":"2024-02-11T00:18:56.336199Z","iopub.status.idle":"2024-02-11T00:21:15.806402Z","shell.execute_reply":"2024-02-11T00:21:15.805348Z","shell.execute_reply.started":"2024-02-11T00:18:56.336494Z"},"papermill":{"duration":null,"end_time":null,"exception":false,"start_time":"2024-02-11T18:16:58.132847","status":"running"},"tags":[],"id":"edeb2490"},"outputs":[],"source":["import json\n","import random\n","import transformers\n","import torch\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","\n","# Load model and tokenizer\n","model_name = \"NousResearch/llama-2-7b-chat-hf\"\n","tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n","model_pipeline = transformers.pipeline(\n","    \"text-generation\",\n","    model=model_name,\n","    torch_dtype=torch.float32,\n","    device_map=\"auto\"  # Use GPU if available\n",")"]},{"cell_type":"code","execution_count":null,"id":"6fa9d8bb","metadata":{"execution":{"iopub.execute_input":"2024-02-11T00:24:50.839130Z","iopub.status.busy":"2024-02-11T00:24:50.838728Z","iopub.status.idle":"2024-02-11T00:24:50.918095Z","shell.execute_reply":"2024-02-11T00:24:50.917057Z","shell.execute_reply.started":"2024-02-11T00:24:50.839098Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"id":"6fa9d8bb"},"outputs":[],"source":["def compute_metrics(y_true, y_pred):\n","    accuracy = accuracy_score(y_true, y_pred)\n","    f1 = f1_score(y_true, y_pred, average='weighted')\n","    precision = precision_score(y_true, y_pred, average='weighted')\n","    recall = recall_score(y_true, y_pred, average='weighted')\n","    return [accuracy, f1, precision, recall]\n","\n","# Check if the response correctly aligns with a given hypothesis\n","def is_response_correct(response_text, correct_hypothesis):\n","    response_text = response_text.lower()\n","    correct_hypothesis_lower = correct_hypothesis.lower()\n","    key_phrases = correct_hypothesis_lower.split()\n","\n","    matching_phrases = [phrase for phrase in key_phrases if phrase in response_text]\n","    match_ratio = len(matching_phrases) / len(key_phrases)\n","\n","    return match_ratio > 0.5  # Adjust this threshold as needed\n","\n","# Load the ecare data from a JSONL file and randomly sample 1000 data points\n","def load_ecare_data(file_path, sample_size=100000):\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        data = json.load(file)\n","\n","    if sample_size >= len(data):\n","        return data\n","\n","    return random.sample(data, sample_size)\n","\n","# Function to calculate F1 score\n","def compute_f1_score(golds, preds):\n","    tp = sum(1 for g, p in zip(golds, preds) if g == 1 and p == 1)\n","    fp = sum(1 for g, p in zip(golds, preds) if g == 0 and p == 1)\n","    fn = sum(1 for g, p in zip(golds, preds) if g == 1 and p == 0)\n","\n","    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n","    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n","    f1score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n","    return f1score\n","\n","# Load the dataset\n","run = 1\n","file_path = '/content/drive/MyDrive/Datasets/causalnetcd.json'\n","responses_file = '/content/drive/MyDrive/Datasets/llama2_causalnet_responses_' + str(run) + '.jsonl'\n","dataset = load_ecare_data(file_path)\n","total_items = len(dataset)\n","preds, golds = [], []\n","\n","# Generate predictions and collect gold labels\n","try:\n","    with open(responses_file, 'r', encoding='utf-8') as file:\n","        saved_responses = [json.loads(line) for line in file]\n","    print(\"Loaded saved responses.\")\n","except FileNotFoundError:\n","    saved_responses = []\n","    for idx, item in enumerate(dataset):\n","        print(f\"\\nProcessing item {idx + 1}/{total_items}...\")\n","        # query = f\"{item['premise']} {item['hypothesis1']} or {item['hypothesis2']}?\"\n","        task = \"cause\" if item[\"ask-for\"] == \"cause\" else \"effect\"\n","        query = f\"{item['context']}; what is the {task}?: 1. {item['choice_id: 0']} OR 2. {item['choice_id: 1']} OR 3. {item['choice_id: 2']}? Answer:\"\n","#         query = f\"Which is more likely to be the {task} for this {item['premise']} - {item['hypothesis1']}(0). or {item['hypothesis2']}(1). Select the right statement. Do not explain yourself.\"\n","        # query = f\"Given the premise {item['premise']} and two possible hypotheses: {item['hypothesis1']} AND {item['hypothesis2']}, which one is the most probable? Pick one verbatim and do not explain yourself.\"\n","        response = model_pipeline(query, do_sample=True, top_k=10, top_p=0.9, temperature=0.2, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id, max_length=150)\n","        generated_text = response[0]['generated_text']\n","        # Sometimes the model repeats the question\n","        if len(generated_text) > len(query):\n","            generated_text = generated_text[len(query):]\n","        print(f\"Generated Response: {generated_text}\")\n","        correct_hypothesis = item['choice_id: ' + str(item['label'])]\n","        print(f\"Correct Hypothesis: hypothesis {item['label'] + 1}, {correct_hypothesis}\")\n","\n","        saved_responses.append({\n","            'index': item['index'],\n","            'premise': item['context'],\n","            'hypothesis1': item['choice_id: 0'],\n","            'hypothesis2': item['choice_id: 1'],\n","            'hypothesis3': item['choice_id: 2'],\n","            'label': item['label'],\n","            'generated_response': generated_text\n","        })\n","        golds.append(item['label'])\n","\n","        if is_response_correct(generated_text, correct_hypothesis):\n","            print(f\"Item {idx}: Correct\")\n","            preds.append(item['label'])\n","        else:\n","            print(f\"Item {idx}: Incorrect\")\n","            preds.append(0 if item['label'] else 1)  # Assuming binary labels (0 and 1)\n","\n","    # Save the generated responses\n","    with open(responses_file, 'w', encoding='utf-8') as file:\n","        for response in saved_responses:\n","            file.write(json.dumps(response) + '\\n')\n","    print(\"Responses saved.\")\n","\n","# If loading from saved file\n","# golds, preds = [], []\n","# for idx, item in enumerate(saved_responses):\n","#     correct_hypothesis = item['hypothesis1'] if item['label'] == 0 else item['hypothesis2']\n","#     generated_text = item['generated_response']\n","#     golds.append(item['label'])\n","#     if is_response_correct(generated_text, correct_hypothesis):\n","#         print(f\"Item {idx}: Correct\")\n","#         preds.append(item['label'])\n","#     else:\n","#         print(f\"Item {idx}: Incorrect\")\n","#         preds.append(1 - item['label'])  # Assuming binary labels (0 and 1)\n","# End if loading from saved file\n","\n","\n","# Calculate F1 score and accuracy\n","f1score = compute_f1_score(golds, preds)\n","accuracy = sum(1 for g, p in zip(golds, preds) if g == p) / len(golds)\n","\n","# Print the results\n","print(f\"F1 Score: {f1score:.2f}\")\n","print(f\"Accuracy: {accuracy:.2f}\")\n","\n","print('---')\n","\n","sklearn_accuracy, f1val, precisionval, recallval = compute_metrics(golds, preds)\n","print(f\"sklearn Accuracy: {sklearn_accuracy * 100:.3f}%\")\n","print(f\"F1: {f1val * 100:.3f}%\")\n","print(f\"Precision: {precisionval * 100:.3f}%\")\n","print(f\"Recall: {recallval * 100:.3f}%\")"]},{"cell_type":"code","execution_count":null,"id":"914e4965","metadata":{"execution":{"iopub.execute_input":"2024-02-11T00:28:47.753665Z","iopub.status.busy":"2024-02-11T00:28:47.753332Z","iopub.status.idle":"2024-02-11T01:21:22.185948Z","shell.execute_reply":"2024-02-11T01:21:22.184990Z","shell.execute_reply.started":"2024-02-11T00:28:47.753642Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"id":"914e4965"},"outputs":[],"source":["def compute_metrics(y_true, y_pred):\n","    accuracy = accuracy_score(y_true, y_pred)\n","    f1 = f1_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    return [accuracy, f1, precision, recall]\n","\n","# Check if the response correctly aligns with a given hypothesis\n","def is_response_correct(response_text, correct_hypothesis):\n","    response_text = response_text.lower()\n","    correct_hypothesis_lower = correct_hypothesis.lower()\n","    key_phrases = correct_hypothesis_lower.split()\n","\n","    matching_phrases = [phrase for phrase in key_phrases if phrase in response_text]\n","    match_ratio = len(matching_phrases) / len(key_phrases)\n","\n","    return match_ratio > 0.5  # Adjust this threshold as needed\n","\n","# Load the ecare data from a JSONL file and randomly sample 1000 data points\n","def load_ecare_data(file_path, sample_size=100000):\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        data = [json.loads(line) for line in file]\n","\n","    if sample_size >= len(data):\n","        return data\n","\n","    return random.sample(data, sample_size)\n","\n","# Function to calculate F1 score\n","def compute_f1_score(golds, preds):\n","    tp = sum(1 for g, p in zip(golds, preds) if g == 1 and p == 1)\n","    fp = sum(1 for g, p in zip(golds, preds) if g == 0 and p == 1)\n","    fn = sum(1 for g, p in zip(golds, preds) if g == 1 and p == 0)\n","\n","    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n","    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n","    f1score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n","    return f1score\n","\n","# Load the dataset\n","run = 2\n","file_path = '/kaggle/input/e-care/ecare_' + str(run) + '.json'\n","responses_file = '/kaggle/working/llama2_responses_' + str(run) + '.jsonl'\n","dataset = load_ecare_data(file_path)\n","total_items = len(dataset)\n","preds, golds = [], []\n","\n","# Generate predictions and collect gold labels\n","try:\n","    with open(responses_file, 'r', encoding='utf-8') as file:\n","        saved_responses = [json.loads(line) for line in file]\n","    print(\"Loaded saved responses.\")\n","except FileNotFoundError:\n","    saved_responses = []\n","    for idx, item in enumerate(dataset):\n","        print(f\"\\nProcessing item {idx + 1}/{total_items}...\")\n","        # query = f\"{item['premise']} {item['hypothesis1']} or {item['hypothesis2']}?\"\n","        task = \"cause\" if item[\"ask-for\"] == \"cause\" else \"effect\"\n","        query = f\"Given the premise {item['premise']}; what is the {task}?: {item['hypothesis1']} OR {item['hypothesis2']}? Answer:\"\n","#         query = f\"Which is more likely to be the {task} for this {item['premise']} - {item['hypothesis1']}(0). or {item['hypothesis2']}(1). Select the right statement. Do not explain yourself.\"\n","        # query = f\"Given the premise {item['premise']} and two possible hypotheses: {item['hypothesis1']} AND {item['hypothesis2']}, which one is the most probable? Pick one verbatim and do not explain yourself.\"\n","        response = model_pipeline(query, do_sample=True, top_k=10, top_p=0.9, temperature=0.2, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id, max_length=100)\n","        generated_text = response[0]['generated_text']\n","        # Sometimes the model repeats the question\n","        if len(generated_text) > len(query):\n","            generated_text = generated_text[len(query):]\n","        print(f\"Generated Response: {generated_text}\")\n","        correct_hypothesis = item['hypothesis1'] if item['label'] == 0 else item['hypothesis2']\n","        print(f\"Correct Hypothesis: hypothesis {item['label'] + 1}, {correct_hypothesis}\")\n","\n","        saved_responses.append({\n","            'index': item['index'],\n","            'premise': item['premise'],\n","            'hypothesis1': item['hypothesis1'],\n","            'hypothesis2': item['hypothesis2'],\n","            'label': item['label'],\n","            'generated_response': generated_text\n","        })\n","        golds.append(item['label'])\n","\n","        if is_response_correct(generated_text, correct_hypothesis):\n","            print(f\"Item {idx}: Correct\")\n","            preds.append(item['label'])\n","        else:\n","            print(f\"Item {idx}: Incorrect\")\n","            preds.append(1 - item['label'])  # Assuming binary labels (0 and 1)\n","\n","    # Save the generated responses\n","    with open(responses_file, 'w', encoding='utf-8') as file:\n","        for response in saved_responses:\n","            file.write(json.dumps(response) + '\\n')\n","    print(\"Responses saved.\")\n","\n","for idx, item in enumerate(saved_responses, 1):\n","    correct_hypothesis = item['hypothesis1'] if item['label'] == 0 else item['hypothesis2']\n","\n","\n","# Calculate F1 score and accuracy\n","f1score = compute_f1_score(golds, preds)\n","accuracy = sum(1 for g, p in zip(golds, preds) if g == p) / len(golds)\n","\n","# Print the results\n","print(f\"F1 Score: {f1score:.2f}\")\n","print(f\"Accuracy: {accuracy:.2f}\")\n","\n","print('---')\n","\n","sklearn_accuracy, f1val, precisionval, recallval = compute_metrics(golds, preds)\n","print(f\"sklearn Accuracy: {sklearn_accuracy * 100:.3f}%\")\n","print(f\"F1: {f1val * 100:.3f}%\")\n","print(f\"Precision: {precisionval * 100:.3f}%\")\n","print(f\"Recall: {recallval * 100:.3f}%\")"]},{"cell_type":"code","execution_count":null,"id":"2be9fc8f","metadata":{"execution":{"iopub.execute_input":"2024-02-11T01:21:22.188499Z","iopub.status.busy":"2024-02-11T01:21:22.188010Z","iopub.status.idle":"2024-02-11T02:14:05.621563Z","shell.execute_reply":"2024-02-11T02:14:05.620461Z","shell.execute_reply.started":"2024-02-11T01:21:22.188466Z"},"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"id":"2be9fc8f"},"outputs":[],"source":["def compute_metrics(y_true, y_pred):\n","    accuracy = accuracy_score(y_true, y_pred)\n","    f1 = f1_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    return [accuracy, f1, precision, recall]\n","\n","# Check if the response correctly aligns with a given hypothesis\n","def is_response_correct(response_text, correct_hypothesis):\n","    response_text = response_text.lower()\n","    correct_hypothesis_lower = correct_hypothesis.lower()\n","    key_phrases = correct_hypothesis_lower.split()\n","\n","    matching_phrases = [phrase for phrase in key_phrases if phrase in response_text]\n","    match_ratio = len(matching_phrases) / len(key_phrases)\n","\n","    return match_ratio > 0.5  # Adjust this threshold as needed\n","\n","# Load the ecare data from a JSONL file and randomly sample 1000 data points\n","def load_ecare_data(file_path, sample_size=100000):\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        data = [json.loads(line) for line in file]\n","\n","    if sample_size >= len(data):\n","        return data\n","\n","    return random.sample(data, sample_size)\n","\n","# Function to calculate F1 score\n","def compute_f1_score(golds, preds):\n","    tp = sum(1 for g, p in zip(golds, preds) if g == 1 and p == 1)\n","    fp = sum(1 for g, p in zip(golds, preds) if g == 0 and p == 1)\n","    fn = sum(1 for g, p in zip(golds, preds) if g == 1 and p == 0)\n","\n","    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n","    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n","    f1score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n","    return f1score\n","\n","# Load the dataset\n","run = 3\n","file_path = '/kaggle/input/e-care/ecare_' + str(run) + '.json'\n","responses_file = '/kaggle/working/llama2_responses_' + str(run) + '.jsonl'\n","dataset = load_ecare_data(file_path)\n","total_items = len(dataset)\n","preds, golds = [], []\n","\n","# Generate predictions and collect gold labels\n","try:\n","    with open(responses_file, 'r', encoding='utf-8') as file:\n","        saved_responses = [json.loads(line) for line in file]\n","    print(\"Loaded saved responses.\")\n","except FileNotFoundError:\n","    saved_responses = []\n","    for idx, item in enumerate(dataset):\n","        print(f\"\\nProcessing item {idx + 1}/{total_items}...\")\n","        # query = f\"{item['premise']} {item['hypothesis1']} or {item['hypothesis2']}?\"\n","        task = \"cause\" if item[\"ask-for\"] == \"cause\" else \"effect\"\n","        query = f\"Given the premise {item['premise']}; what is the {task}?: {item['hypothesis1']} OR {item['hypothesis2']}? Answer:\"\n","#         query = f\"Which is more likely to be the {task} for this {item['premise']} - {item['hypothesis1']}(0). or {item['hypothesis2']}(1). Select the right statement. Do not explain yourself.\"\n","        # query = f\"Given the premise {item['premise']} and two possible hypotheses: {item['hypothesis1']} AND {item['hypothesis2']}, which one is the most probable? Pick one verbatim and do not explain yourself.\"\n","        response = model_pipeline(query, do_sample=True, top_k=10, top_p=0.9, temperature=0.2, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id, max_length=100)\n","        generated_text = response[0]['generated_text']\n","        # Sometimes the model repeats the question\n","        if len(generated_text) > len(query):\n","            generated_text = generated_text[len(query):]\n","        print(f\"Generated Response: {generated_text}\")\n","        correct_hypothesis = item['hypothesis1'] if item['label'] == 0 else item['hypothesis2']\n","        print(f\"Correct Hypothesis: hypothesis {item['label'] + 1}, {correct_hypothesis}\")\n","\n","        saved_responses.append({\n","            'index': item['index'],\n","            'premise': item['premise'],\n","            'hypothesis1': item['hypothesis1'],\n","            'hypothesis2': item['hypothesis2'],\n","            'label': item['label'],\n","            'generated_response': generated_text\n","        })\n","        golds.append(item['label'])\n","\n","        if is_response_correct(generated_text, correct_hypothesis):\n","            print(f\"Item {idx}: Correct\")\n","            preds.append(item['label'])\n","        else:\n","            print(f\"Item {idx}: Incorrect\")\n","            preds.append(1 - item['label'])  # Assuming binary labels (0 and 1)\n","\n","    # Save the generated responses\n","    with open(responses_file, 'w', encoding='utf-8') as file:\n","        for response in saved_responses:\n","            file.write(json.dumps(response) + '\\n')\n","    print(\"Responses saved.\")\n","\n","for idx, item in enumerate(saved_responses, 1):\n","    correct_hypothesis = item['hypothesis1'] if item['label'] == 0 else item['hypothesis2']\n","\n","\n","# Calculate F1 score and accuracy\n","f1score = compute_f1_score(golds, preds)\n","accuracy = sum(1 for g, p in zip(golds, preds) if g == p) / len(golds)\n","\n","# Print the results\n","print(f\"F1 Score: {f1score:.2f}\")\n","print(f\"Accuracy: {accuracy:.2f}\")\n","\n","print('---')\n","\n","sklearn_accuracy, f1val, precisionval, recallval = compute_metrics(golds, preds)\n","print(f\"sklearn Accuracy: {sklearn_accuracy * 100:.3f}%\")\n","print(f\"F1: {f1val * 100:.3f}%\")\n","print(f\"Precision: {precisionval * 100:.3f}%\")\n","print(f\"Recall: {recallval * 100:.3f}%\")"]},{"cell_type":"code","execution_count":null,"id":"4f1aff60","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"id":"4f1aff60"},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4422717,"sourceId":7597894,"sourceType":"datasetVersion"}],"dockerImageVersionId":30648,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":null,"end_time":null,"environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-11T18:16:55.109265","version":"2.5.0"},"colab":{"provenance":[],"machine_shape":"hm"}},"nbformat":4,"nbformat_minor":5}