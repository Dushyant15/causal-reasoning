{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dy4P0ZLS_45N",
        "outputId": "f434ca29-8553-4186-f97b-1dfe67a43ebc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 1.1063674688339233\n",
            "Epoch: 0, Loss: 1.1179121732711792\n",
            "Epoch: 0, Loss: 1.0971485376358032\n",
            "Epoch: 0, Loss: 1.091286540031433\n",
            "Epoch: 0, Loss: 1.0939255952835083\n",
            "Epoch: 0, Loss: 1.076003909111023\n",
            "Epoch: 0, Loss: 1.0934382677078247\n",
            "Epoch: 0, Loss: 1.0762399435043335\n",
            "Epoch: 0, Loss: 1.0819300413131714\n",
            "Epoch: 0, Loss: 1.131539225578308\n",
            "Epoch: 0, Loss: 1.1313045024871826\n",
            "Epoch: 0, Loss: 1.1189215183258057\n",
            "Epoch: 0, Loss: 1.1512296199798584\n",
            "Epoch: 0, Loss: 1.073452353477478\n",
            "Epoch: 0, Loss: 1.1246238946914673\n",
            "Epoch: 0, Loss: 1.1163668632507324\n",
            "Epoch: 0, Loss: 1.0931422710418701\n",
            "Epoch: 0, Loss: 1.1109298467636108\n",
            "Epoch: 0, Loss: 1.116931676864624\n",
            "Epoch: 0, Loss: 1.102855920791626\n",
            "Epoch: 0, Loss: 1.113486647605896\n",
            "Epoch: 0, Loss: 1.1127877235412598\n",
            "Epoch: 0, Loss: 1.0840166807174683\n",
            "Epoch: 0, Loss: 1.0991486310958862\n",
            "Epoch: 0, Loss: 1.0791090726852417\n",
            "Epoch: 0, Loss: 1.1009759902954102\n",
            "Epoch: 0, Loss: 1.0807230472564697\n",
            "Epoch: 0, Loss: 1.0796624422073364\n",
            "Epoch: 0, Loss: 1.1088567972183228\n",
            "Epoch: 0, Loss: 1.114112377166748\n",
            "Epoch: 0, Loss: 1.103337049484253\n",
            "Epoch: 0, Loss: 1.0829154253005981\n",
            "Epoch: 0, Loss: 1.107923150062561\n",
            "Epoch: 0, Loss: 1.103732943534851\n",
            "Epoch: 0, Loss: 1.1121243238449097\n",
            "Epoch: 0, Loss: 1.0884019136428833\n",
            "Epoch: 0, Loss: 1.104003667831421\n",
            "Epoch: 0, Loss: 1.0951693058013916\n",
            "Epoch: 0, Loss: 1.123417615890503\n",
            "Epoch: 0, Loss: 1.0895652770996094\n",
            "Epoch: 0, Loss: 1.0723209381103516\n",
            "Epoch: 0, Loss: 1.1273741722106934\n",
            "Epoch: 0, Loss: 1.1221612691879272\n",
            "Epoch: 0, Loss: 1.066999077796936\n",
            "Epoch: 0, Loss: 1.1004164218902588\n",
            "Epoch: 0, Loss: 1.077048897743225\n",
            "Epoch: 0, Loss: 1.079114556312561\n",
            "Epoch: 0, Loss: 1.1368932723999023\n",
            "Epoch: 0, Loss: 1.0741702318191528\n",
            "Epoch: 0, Loss: 1.1010669469833374\n",
            "Epoch: 0, Loss: 1.1175676584243774\n",
            "Epoch: 0, Loss: 1.0895605087280273\n",
            "Epoch: 0, Loss: 1.1185457706451416\n",
            "Epoch: 0, Loss: 1.111444354057312\n",
            "Epoch: 0, Loss: 1.0883162021636963\n",
            "Epoch: 0, Loss: 1.0945724248886108\n",
            "Epoch: 0, Loss: 1.0541960000991821\n",
            "Epoch: 0, Loss: 1.0682281255722046\n",
            "Epoch: 0, Loss: 1.0989145040512085\n",
            "Epoch: 0, Loss: 1.1327913999557495\n",
            "Epoch: 0, Loss: 1.129880666732788\n",
            "Epoch: 0, Loss: 1.1124846935272217\n",
            "Epoch: 0, Loss: 1.1601022481918335\n",
            "Epoch: 0, Loss: 1.0994592905044556\n",
            "Epoch: 0, Loss: 1.1011759042739868\n",
            "Epoch: 0, Loss: 1.0452549457550049\n",
            "Epoch: 0, Loss: 1.1018489599227905\n",
            "Epoch: 0, Loss: 1.1011942625045776\n",
            "Epoch: 0, Loss: 1.108577847480774\n",
            "Epoch: 0, Loss: 1.138686180114746\n",
            "Epoch: 0, Loss: 1.102543830871582\n",
            "Epoch: 0, Loss: 1.0922242403030396\n",
            "Epoch: 0, Loss: 1.0666247606277466\n",
            "Epoch: 0, Loss: 1.1510655879974365\n",
            "Epoch: 0, Loss: 1.0663892030715942\n",
            "Epoch: 0, Loss: 1.0845052003860474\n",
            "Epoch: 0, Loss: 1.0898820161819458\n",
            "Epoch: 0, Loss: 1.1010931730270386\n",
            "Epoch: 0, Loss: 1.1339154243469238\n",
            "Epoch: 0, Loss: 1.0908769369125366\n",
            "Epoch: 0, Loss: 1.0883792638778687\n",
            "Epoch: 0, Loss: 1.1032971143722534\n",
            "Epoch: 0, Loss: 1.1202362775802612\n",
            "Epoch: 0, Loss: 1.1047435998916626\n",
            "Epoch: 0, Loss: 1.1292014122009277\n",
            "Epoch: 0, Loss: 1.0969706773757935\n",
            "Epoch: 0, Loss: 1.1211775541305542\n",
            "Epoch: 0, Loss: 1.126853346824646\n",
            "Epoch: 0, Loss: 1.08584463596344\n",
            "Epoch: 0, Loss: 1.0945249795913696\n",
            "Epoch: 0, Loss: 1.1351549625396729\n",
            "Epoch: 0, Loss: 1.0808981657028198\n",
            "Epoch: 0, Loss: 1.0826233625411987\n",
            "Epoch: 0, Loss: 1.1246031522750854\n",
            "Epoch: 0, Loss: 1.102047324180603\n",
            "Epoch: 0, Loss: 1.1093448400497437\n",
            "Epoch: 0, Loss: 1.094555139541626\n",
            "Epoch: 0, Loss: 1.0970721244812012\n",
            "Epoch: 0, Loss: 1.1047414541244507\n",
            "Epoch: 0, Loss: 1.1062456369400024\n",
            "Epoch: 1, Loss: 1.0989100933074951\n",
            "Epoch: 1, Loss: 1.0962187051773071\n",
            "Epoch: 1, Loss: 1.0919841527938843\n",
            "Epoch: 1, Loss: 1.0943820476531982\n",
            "Epoch: 1, Loss: 1.0946117639541626\n",
            "Epoch: 1, Loss: 1.095399260520935\n",
            "Epoch: 1, Loss: 1.0834553241729736\n",
            "Epoch: 1, Loss: 1.0955698490142822\n",
            "Epoch: 1, Loss: 1.100107192993164\n",
            "Epoch: 1, Loss: 1.0853384733200073\n",
            "Epoch: 1, Loss: 1.105845332145691\n",
            "Epoch: 1, Loss: 1.1046738624572754\n",
            "Epoch: 1, Loss: 1.0907437801361084\n",
            "Epoch: 1, Loss: 1.102040410041809\n",
            "Epoch: 1, Loss: 1.1041380167007446\n",
            "Epoch: 1, Loss: 1.140818476676941\n",
            "Epoch: 1, Loss: 1.1026924848556519\n",
            "Epoch: 1, Loss: 1.0987671613693237\n",
            "Epoch: 1, Loss: 1.0949333906173706\n",
            "Epoch: 1, Loss: 1.128023386001587\n",
            "Epoch: 1, Loss: 1.11979341506958\n",
            "Epoch: 1, Loss: 1.1108678579330444\n",
            "Epoch: 1, Loss: 1.1059293746948242\n",
            "Epoch: 1, Loss: 1.1148329973220825\n",
            "Epoch: 1, Loss: 1.1069585084915161\n",
            "Epoch: 1, Loss: 1.11375892162323\n",
            "Epoch: 1, Loss: 1.0796431303024292\n",
            "Epoch: 1, Loss: 1.1053982973098755\n",
            "Epoch: 1, Loss: 1.1114550828933716\n",
            "Epoch: 1, Loss: 1.1121755838394165\n",
            "Epoch: 1, Loss: 1.105570673942566\n",
            "Epoch: 1, Loss: 1.0653103590011597\n",
            "Epoch: 1, Loss: 1.1183147430419922\n",
            "Epoch: 1, Loss: 1.0907315015792847\n",
            "Epoch: 1, Loss: 1.076576828956604\n",
            "Epoch: 1, Loss: 1.1373623609542847\n",
            "Epoch: 1, Loss: 1.104282021522522\n",
            "Epoch: 1, Loss: 1.0829894542694092\n",
            "Epoch: 1, Loss: 1.1340934038162231\n",
            "Epoch: 1, Loss: 1.123341679573059\n",
            "Epoch: 1, Loss: 1.1001557111740112\n",
            "Epoch: 1, Loss: 1.0623823404312134\n",
            "Epoch: 1, Loss: 1.0851885080337524\n",
            "Epoch: 1, Loss: 1.0887211561203003\n",
            "Epoch: 1, Loss: 1.134414553642273\n",
            "Epoch: 1, Loss: 1.1142290830612183\n",
            "Epoch: 1, Loss: 1.0943266153335571\n",
            "Epoch: 1, Loss: 1.0663115978240967\n",
            "Epoch: 1, Loss: 1.109942078590393\n",
            "Epoch: 1, Loss: 1.09565269947052\n",
            "Epoch: 1, Loss: 1.1002488136291504\n",
            "Epoch: 1, Loss: 1.09048593044281\n",
            "Epoch: 1, Loss: 1.1113297939300537\n",
            "Epoch: 1, Loss: 1.1078085899353027\n",
            "Epoch: 1, Loss: 1.1167808771133423\n",
            "Epoch: 1, Loss: 1.101379156112671\n",
            "Epoch: 1, Loss: 1.0877217054367065\n",
            "Epoch: 1, Loss: 1.1366881132125854\n",
            "Epoch: 1, Loss: 1.0841647386550903\n",
            "Epoch: 1, Loss: 1.0894869565963745\n",
            "Epoch: 1, Loss: 1.1272763013839722\n",
            "Epoch: 1, Loss: 1.0975110530853271\n",
            "Epoch: 1, Loss: 1.09174382686615\n",
            "Epoch: 1, Loss: 1.1010562181472778\n",
            "Epoch: 1, Loss: 1.0990614891052246\n",
            "Epoch: 1, Loss: 1.0934430360794067\n",
            "Epoch: 1, Loss: 1.1223002672195435\n",
            "Epoch: 1, Loss: 1.106039047241211\n",
            "Epoch: 1, Loss: 1.1058218479156494\n",
            "Epoch: 1, Loss: 1.0927863121032715\n",
            "Epoch: 1, Loss: 1.0895036458969116\n",
            "Epoch: 1, Loss: 1.099575400352478\n",
            "Epoch: 1, Loss: 1.1019816398620605\n",
            "Epoch: 1, Loss: 1.075583577156067\n",
            "Epoch: 1, Loss: 1.0883312225341797\n",
            "Epoch: 1, Loss: 1.0871495008468628\n",
            "Epoch: 1, Loss: 1.0824042558670044\n",
            "Epoch: 1, Loss: 1.0963927507400513\n",
            "Epoch: 1, Loss: 1.0913089513778687\n",
            "Epoch: 1, Loss: 1.1103792190551758\n",
            "Epoch: 1, Loss: 1.0991333723068237\n",
            "Epoch: 1, Loss: 1.1053863763809204\n",
            "Epoch: 1, Loss: 1.0507323741912842\n",
            "Epoch: 1, Loss: 1.1265395879745483\n",
            "Epoch: 1, Loss: 1.0874481201171875\n",
            "Epoch: 1, Loss: 1.0969083309173584\n",
            "Epoch: 1, Loss: 1.1560521125793457\n",
            "Epoch: 1, Loss: 1.1089060306549072\n",
            "Epoch: 1, Loss: 1.104597568511963\n",
            "Epoch: 1, Loss: 1.1299818754196167\n",
            "Epoch: 1, Loss: 1.0944797992706299\n",
            "Epoch: 1, Loss: 1.0756901502609253\n",
            "Epoch: 1, Loss: 1.151771903038025\n",
            "Epoch: 1, Loss: 1.136817455291748\n",
            "Epoch: 1, Loss: 1.07736337184906\n",
            "Epoch: 1, Loss: 1.0845509767532349\n",
            "Epoch: 1, Loss: 1.1102818250656128\n",
            "Epoch: 1, Loss: 1.1137536764144897\n",
            "Epoch: 1, Loss: 1.0674211978912354\n",
            "Epoch: 1, Loss: 1.0643712282180786\n",
            "Epoch: 2, Loss: 1.1166926622390747\n",
            "Epoch: 2, Loss: 1.1117136478424072\n",
            "Epoch: 2, Loss: 1.105503797531128\n",
            "Epoch: 2, Loss: 1.1157580614089966\n",
            "Epoch: 2, Loss: 1.1157017946243286\n",
            "Epoch: 2, Loss: 1.0893298387527466\n",
            "Epoch: 2, Loss: 1.111171841621399\n",
            "Epoch: 2, Loss: 1.1271271705627441\n",
            "Epoch: 2, Loss: 1.089365839958191\n",
            "Epoch: 2, Loss: 1.1119316816329956\n",
            "Epoch: 2, Loss: 1.0690701007843018\n",
            "Epoch: 2, Loss: 1.0990320444107056\n",
            "Epoch: 2, Loss: 1.1208484172821045\n",
            "Epoch: 2, Loss: 1.125759482383728\n",
            "Epoch: 2, Loss: 1.074620246887207\n",
            "Epoch: 2, Loss: 1.107704520225525\n",
            "Epoch: 2, Loss: 1.0952357053756714\n",
            "Epoch: 2, Loss: 1.098608136177063\n",
            "Epoch: 2, Loss: 1.0823395252227783\n",
            "Epoch: 2, Loss: 1.0679574012756348\n",
            "Epoch: 2, Loss: 1.0729293823242188\n",
            "Epoch: 2, Loss: 1.080309510231018\n",
            "Epoch: 2, Loss: 1.1103533506393433\n",
            "Epoch: 2, Loss: 1.1204317808151245\n",
            "Epoch: 2, Loss: 1.0987142324447632\n",
            "Epoch: 2, Loss: 1.1228889226913452\n",
            "Epoch: 2, Loss: 1.1313549280166626\n",
            "Epoch: 2, Loss: 1.1099566221237183\n",
            "Epoch: 2, Loss: 1.1293643712997437\n",
            "Epoch: 2, Loss: 1.1550393104553223\n",
            "Epoch: 2, Loss: 1.099765419960022\n",
            "Epoch: 2, Loss: 1.0770866870880127\n",
            "Epoch: 2, Loss: 1.0862077474594116\n",
            "Epoch: 2, Loss: 1.1035313606262207\n",
            "Epoch: 2, Loss: 1.0938442945480347\n",
            "Epoch: 2, Loss: 1.0800985097885132\n",
            "Epoch: 2, Loss: 1.0973330736160278\n",
            "Epoch: 2, Loss: 1.1148029565811157\n",
            "Epoch: 2, Loss: 1.1285111904144287\n",
            "Epoch: 2, Loss: 1.109770655632019\n",
            "Epoch: 2, Loss: 1.1109572649002075\n",
            "Epoch: 2, Loss: 1.0830693244934082\n",
            "Epoch: 2, Loss: 1.1143423318862915\n",
            "Epoch: 2, Loss: 1.1016498804092407\n",
            "Epoch: 2, Loss: 1.0997703075408936\n",
            "Epoch: 2, Loss: 1.0914933681488037\n",
            "Epoch: 2, Loss: 1.1032613515853882\n",
            "Epoch: 2, Loss: 1.0693482160568237\n",
            "Epoch: 2, Loss: 1.0753763914108276\n",
            "Epoch: 2, Loss: 1.0979304313659668\n",
            "Epoch: 2, Loss: 1.1053197383880615\n",
            "Epoch: 2, Loss: 1.1157630681991577\n",
            "Epoch: 2, Loss: 1.0919229984283447\n",
            "Epoch: 2, Loss: 1.1227927207946777\n",
            "Epoch: 2, Loss: 1.088761806488037\n",
            "Epoch: 2, Loss: 1.0825997591018677\n",
            "Epoch: 2, Loss: 1.100091576576233\n",
            "Epoch: 2, Loss: 1.0954574346542358\n",
            "Epoch: 2, Loss: 1.1010979413986206\n",
            "Epoch: 2, Loss: 1.1090776920318604\n",
            "Epoch: 2, Loss: 1.144140601158142\n",
            "Epoch: 2, Loss: 1.1147688627243042\n",
            "Epoch: 2, Loss: 1.089939832687378\n",
            "Epoch: 2, Loss: 1.1068731546401978\n",
            "Epoch: 2, Loss: 1.098954439163208\n",
            "Epoch: 2, Loss: 1.1090108156204224\n",
            "Epoch: 2, Loss: 1.08039391040802\n",
            "Epoch: 2, Loss: 1.0942442417144775\n",
            "Epoch: 2, Loss: 1.0923911333084106\n",
            "Epoch: 2, Loss: 1.1028234958648682\n",
            "Epoch: 2, Loss: 1.1011813879013062\n",
            "Epoch: 2, Loss: 1.0931488275527954\n",
            "Epoch: 2, Loss: 1.1016725301742554\n",
            "Epoch: 2, Loss: 1.1109848022460938\n",
            "Epoch: 2, Loss: 1.091044545173645\n",
            "Epoch: 2, Loss: 1.0798133611679077\n",
            "Epoch: 2, Loss: 1.1334844827651978\n",
            "Epoch: 2, Loss: 1.0810524225234985\n",
            "Epoch: 2, Loss: 1.1282933950424194\n",
            "Epoch: 2, Loss: 1.0910569429397583\n",
            "Epoch: 2, Loss: 1.0930298566818237\n",
            "Epoch: 2, Loss: 1.1046828031539917\n",
            "Epoch: 2, Loss: 1.1291117668151855\n",
            "Epoch: 2, Loss: 1.1369117498397827\n",
            "Epoch: 2, Loss: 1.0979022979736328\n",
            "Epoch: 2, Loss: 1.115089774131775\n",
            "Epoch: 2, Loss: 1.0994480848312378\n",
            "Epoch: 2, Loss: 1.0899899005889893\n",
            "Epoch: 2, Loss: 1.0940526723861694\n",
            "Epoch: 2, Loss: 1.101224660873413\n",
            "Epoch: 2, Loss: 1.0902281999588013\n",
            "Epoch: 2, Loss: 1.1185370683670044\n",
            "Epoch: 2, Loss: 1.1101516485214233\n",
            "Epoch: 2, Loss: 1.1067832708358765\n",
            "Epoch: 2, Loss: 1.0524790287017822\n",
            "Epoch: 2, Loss: 1.1178399324417114\n",
            "Epoch: 2, Loss: 1.1181061267852783\n",
            "Epoch: 2, Loss: 1.0807526111602783\n",
            "Epoch: 2, Loss: 1.08863365650177\n",
            "Epoch: 2, Loss: 1.1161938905715942\n",
            "Accuracy: 0.3800\n",
            "F1: 0.2093\n",
            "Precision: 0.1444\n",
            "Recall: 0.3800\n",
            "ROC-AUC: 0.4517\n",
            "MCC: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import torch\n",
        "import json\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, matthews_corrcoef, confusion_matrix\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Load the data\n",
        "with open('/content/casualnet.json', 'r') as f:\n",
        "    casulnet_data = json.load(f)\n",
        "\n",
        "\n",
        "random_selected_samples = random.sample(casulnet_data, 1000)\n",
        "\n",
        "# Define split sizes for an 80-20 split\n",
        "train_size = int(0.8 * len(random_selected_samples))\n",
        "train_data = random_selected_samples[:train_size]\n",
        "validation_data = random_selected_samples[train_size:]\n",
        "\n",
        "class CasulnetDataset(Dataset):\n",
        "    def __init__(self, casulnet_data, tokenizer):\n",
        "        self.casulnet_data = casulnet_data\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.casulnet_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = self.casulnet_data[idx]\n",
        "        context_question = data['context']\n",
        "        choices = [data[f'choice_id: {i}'] for i in range(3)]  # Assuming there are always 3 choices\n",
        "        label = data['label']\n",
        "\n",
        "        # Tokenizing context_question with each choice\n",
        "        input_ids = []\n",
        "        attention_masks = []\n",
        "        for choice in choices:\n",
        "            encoded_input = self.tokenizer(context_question, choice, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
        "            input_ids.append(encoded_input['input_ids'].squeeze(0))\n",
        "            attention_masks.append(encoded_input['attention_mask'].squeeze(0))\n",
        "\n",
        "        label = torch.tensor([label]*3)  # Replicate label for each choice\n",
        "        return {\n",
        "            'input_ids': torch.stack(input_ids),\n",
        "            'attention_mask': torch.stack(attention_masks),\n",
        "            'labels': label\n",
        "        }\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=3)  # Assuming 3 possible labels\n",
        "\n",
        "casulnet_dataset = CasulnetDataset(train_data, tokenizer)\n",
        "dataloader = DataLoader(casulnet_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "for epoch in range(3):\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].view(-1, 512)  # Flatten the input for processing\n",
        "        attention_mask = batch['attention_mask'].view(-1, 512)  # Flatten the attention mask for processing\n",
        "        labels = batch['labels'].view(-1)  # Flatten the labels to match the input batch size\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
        "\n",
        "validation_dataset = CasulnetDataset(validation_data, tokenizer)\n",
        "validation_dataloader = DataLoader(validation_dataset, batch_size=64)\n",
        "\n",
        "model.eval()\n",
        "y_true, y_pred, y_scores = [], [], []\n",
        "\n",
        "def compute_metrics(y_true, y_pred, y_prob):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "    precision = precision_score(y_true, y_pred, average='weighted')\n",
        "    recall = recall_score(y_true, y_pred, average='weighted')\n",
        "    roc_auc = roc_auc_score(label_binarize(y_true, classes=[0, 1, 2]), y_prob, multi_class='ovr', average='weighted')\n",
        "    mcc = matthews_corrcoef(y_true, y_pred)\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    return accuracy, f1, precision, recall, roc_auc, mcc, conf_matrix\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in validation_dataloader:\n",
        "        input_ids = batch['input_ids'].view(-1, 512)\n",
        "        attention_mask = batch['attention_mask'].view(-1, 512)\n",
        "        labels = batch['labels'].view(-1)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs.logits, dim=1)\n",
        "        y_scores_batch = torch.softmax(outputs.logits, dim=1).cpu().numpy()\n",
        "\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(predictions.cpu().numpy())\n",
        "        y_scores.extend(y_scores_batch)\n",
        "\n",
        "# Calculate metrics\n",
        "y_scores = np.vstack(y_scores)  # Ensure y_scores is properly shaped for multiclass ROC-AUC calculation\n",
        "accuracy, f1, precision, recall, roc_auc, mcc, conf_matrix = compute_metrics(y_true, y_pred, y_scores)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"F1: {f1:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
        "print(f\"MCC: {mcc:.4f}\")\n"
      ]
    }
  ]
}