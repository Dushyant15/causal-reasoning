{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bx0rVOeAMVPl",
        "outputId": "a665653e-7ec9-4d82-f82a-2cfaaa928dda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 1.1563962697982788\n",
            "Epoch: 0, Loss: 1.2075973749160767\n",
            "Epoch: 0, Loss: 1.12165367603302\n",
            "Epoch: 0, Loss: 1.1191492080688477\n",
            "Epoch: 0, Loss: 1.1691914796829224\n",
            "Epoch: 0, Loss: 1.264025330543518\n",
            "Epoch: 0, Loss: 1.1338104009628296\n",
            "Epoch: 0, Loss: 1.1013582944869995\n",
            "Epoch: 0, Loss: 1.0892025232315063\n",
            "Epoch: 0, Loss: 1.100443720817566\n",
            "Epoch: 0, Loss: 1.0875569581985474\n",
            "Epoch: 0, Loss: 1.0420490503311157\n",
            "Epoch: 0, Loss: 1.0447098016738892\n",
            "Epoch: 0, Loss: 1.0327993631362915\n",
            "Epoch: 0, Loss: 1.147185206413269\n",
            "Epoch: 0, Loss: 0.9357431530952454\n",
            "Epoch: 0, Loss: 1.11296808719635\n",
            "Epoch: 0, Loss: 1.3347187042236328\n",
            "Epoch: 0, Loss: 0.973828136920929\n",
            "Epoch: 0, Loss: 1.2323087453842163\n",
            "Epoch: 0, Loss: 0.9776803851127625\n",
            "Epoch: 0, Loss: 1.0307824611663818\n",
            "Epoch: 0, Loss: 1.168930172920227\n",
            "Epoch: 0, Loss: 1.2726839780807495\n",
            "Epoch: 0, Loss: 0.9637457728385925\n",
            "Epoch: 0, Loss: 1.1517966985702515\n",
            "Epoch: 0, Loss: 1.129158854484558\n",
            "Epoch: 0, Loss: 0.9343517422676086\n",
            "Epoch: 0, Loss: 0.9750391840934753\n",
            "Epoch: 0, Loss: 1.1786423921585083\n",
            "Epoch: 0, Loss: 1.0755738019943237\n",
            "Epoch: 0, Loss: 1.2405647039413452\n",
            "Epoch: 0, Loss: 0.9911339282989502\n",
            "Epoch: 0, Loss: 1.2807587385177612\n",
            "Epoch: 0, Loss: 0.9449041485786438\n",
            "Epoch: 0, Loss: 1.0354658365249634\n",
            "Epoch: 0, Loss: 1.0636022090911865\n",
            "Epoch: 0, Loss: 1.118481159210205\n",
            "Epoch: 0, Loss: 1.1205726861953735\n",
            "Epoch: 0, Loss: 1.2441562414169312\n",
            "Epoch: 0, Loss: 1.0872108936309814\n",
            "Epoch: 0, Loss: 1.0504536628723145\n",
            "Epoch: 0, Loss: 1.2059086561203003\n",
            "Epoch: 0, Loss: 1.0942286252975464\n",
            "Epoch: 0, Loss: 1.1480333805084229\n",
            "Epoch: 0, Loss: 1.1844875812530518\n",
            "Epoch: 0, Loss: 1.1107257604599\n",
            "Epoch: 0, Loss: 1.0733767747879028\n",
            "Epoch: 0, Loss: 1.0701727867126465\n",
            "Epoch: 0, Loss: 1.1765280961990356\n",
            "Epoch: 0, Loss: 1.148977279663086\n",
            "Epoch: 0, Loss: 1.1609071493148804\n",
            "Epoch: 0, Loss: 1.0725606679916382\n",
            "Epoch: 0, Loss: 1.1204079389572144\n",
            "Epoch: 0, Loss: 1.140026569366455\n",
            "Epoch: 0, Loss: 1.0869719982147217\n",
            "Epoch: 0, Loss: 1.0840626955032349\n",
            "Epoch: 0, Loss: 1.1049221754074097\n",
            "Epoch: 0, Loss: 1.1221436262130737\n",
            "Epoch: 0, Loss: 1.1053738594055176\n",
            "Epoch: 0, Loss: 1.1260961294174194\n",
            "Epoch: 0, Loss: 1.0830597877502441\n",
            "Epoch: 0, Loss: 1.1720350980758667\n",
            "Epoch: 0, Loss: 1.0689173936843872\n",
            "Epoch: 0, Loss: 1.125276803970337\n",
            "Epoch: 0, Loss: 1.1349996328353882\n",
            "Epoch: 0, Loss: 1.0896072387695312\n",
            "Epoch: 0, Loss: 1.0461689233779907\n",
            "Epoch: 0, Loss: 1.094408631324768\n",
            "Epoch: 0, Loss: 1.1292263269424438\n",
            "Epoch: 0, Loss: 1.2462078332901\n",
            "Epoch: 0, Loss: 1.1734713315963745\n",
            "Epoch: 0, Loss: 1.0136065483093262\n",
            "Epoch: 0, Loss: 1.1935670375823975\n",
            "Epoch: 0, Loss: 1.095622181892395\n",
            "Epoch: 0, Loss: 1.181937575340271\n",
            "Epoch: 0, Loss: 1.1037451028823853\n",
            "Epoch: 0, Loss: 1.1007250547409058\n",
            "Epoch: 0, Loss: 1.0911182165145874\n",
            "Epoch: 0, Loss: 1.074586033821106\n",
            "Epoch: 0, Loss: 1.1288295984268188\n",
            "Epoch: 0, Loss: 1.1817470788955688\n",
            "Epoch: 0, Loss: 1.084513545036316\n",
            "Epoch: 0, Loss: 1.1323680877685547\n",
            "Epoch: 0, Loss: 1.0572881698608398\n",
            "Epoch: 0, Loss: 1.0614134073257446\n",
            "Epoch: 0, Loss: 1.0389293432235718\n",
            "Epoch: 0, Loss: 1.0463875532150269\n",
            "Epoch: 0, Loss: 1.171138882637024\n",
            "Epoch: 0, Loss: 1.2005666494369507\n",
            "Epoch: 0, Loss: 1.290023922920227\n",
            "Epoch: 0, Loss: 1.2327958345413208\n",
            "Epoch: 0, Loss: 1.1828727722167969\n",
            "Epoch: 0, Loss: 1.1193007230758667\n",
            "Epoch: 0, Loss: 1.080064296722412\n",
            "Epoch: 0, Loss: 1.0586371421813965\n",
            "Epoch: 0, Loss: 1.0787018537521362\n",
            "Epoch: 0, Loss: 1.0942409038543701\n",
            "Epoch: 0, Loss: 1.0571622848510742\n",
            "Epoch: 0, Loss: 1.1115731000900269\n",
            "Epoch: 1, Loss: 1.0750770568847656\n",
            "Epoch: 1, Loss: 1.0744487047195435\n",
            "Epoch: 1, Loss: 1.0675171613693237\n",
            "Epoch: 1, Loss: 1.1734223365783691\n",
            "Epoch: 1, Loss: 1.033960223197937\n",
            "Epoch: 1, Loss: 1.1616069078445435\n",
            "Epoch: 1, Loss: 1.1237714290618896\n",
            "Epoch: 1, Loss: 1.1868404150009155\n",
            "Epoch: 1, Loss: 1.048378586769104\n",
            "Epoch: 1, Loss: 1.1608332395553589\n",
            "Epoch: 1, Loss: 1.1219230890274048\n",
            "Epoch: 1, Loss: 1.1247414350509644\n",
            "Epoch: 1, Loss: 1.0272163152694702\n",
            "Epoch: 1, Loss: 1.119990587234497\n",
            "Epoch: 1, Loss: 1.0743564367294312\n",
            "Epoch: 1, Loss: 1.147013783454895\n",
            "Epoch: 1, Loss: 1.0932769775390625\n",
            "Epoch: 1, Loss: 1.0352057218551636\n",
            "Epoch: 1, Loss: 1.1261571645736694\n",
            "Epoch: 1, Loss: 1.2004938125610352\n",
            "Epoch: 1, Loss: 1.0466006994247437\n",
            "Epoch: 1, Loss: 1.048265814781189\n",
            "Epoch: 1, Loss: 1.040891170501709\n",
            "Epoch: 1, Loss: 1.0607776641845703\n",
            "Epoch: 1, Loss: 1.082837700843811\n",
            "Epoch: 1, Loss: 1.1430145502090454\n",
            "Epoch: 1, Loss: 1.1354379653930664\n",
            "Epoch: 1, Loss: 1.2011052370071411\n",
            "Epoch: 1, Loss: 1.179526448249817\n",
            "Epoch: 1, Loss: 1.2128466367721558\n",
            "Epoch: 1, Loss: 1.09525728225708\n",
            "Epoch: 1, Loss: 1.058918833732605\n",
            "Epoch: 1, Loss: 1.0863591432571411\n",
            "Epoch: 1, Loss: 1.1422356367111206\n",
            "Epoch: 1, Loss: 1.1047500371932983\n",
            "Epoch: 1, Loss: 1.0459953546524048\n",
            "Epoch: 1, Loss: 1.1527397632598877\n",
            "Epoch: 1, Loss: 1.0786060094833374\n",
            "Epoch: 1, Loss: 1.1437991857528687\n",
            "Epoch: 1, Loss: 1.0919456481933594\n",
            "Epoch: 1, Loss: 1.0824954509735107\n",
            "Epoch: 1, Loss: 1.1198502779006958\n",
            "Epoch: 1, Loss: 1.1773070096969604\n",
            "Epoch: 1, Loss: 1.0644608736038208\n",
            "Epoch: 1, Loss: 1.0625205039978027\n",
            "Epoch: 1, Loss: 1.0665602684020996\n",
            "Epoch: 1, Loss: 1.0697479248046875\n",
            "Epoch: 1, Loss: 1.1317219734191895\n",
            "Epoch: 1, Loss: 1.1277717351913452\n",
            "Epoch: 1, Loss: 1.0348033905029297\n",
            "Epoch: 1, Loss: 1.04762601852417\n",
            "Epoch: 1, Loss: 1.087717890739441\n",
            "Epoch: 1, Loss: 1.0443044900894165\n",
            "Epoch: 1, Loss: 1.221204400062561\n",
            "Epoch: 1, Loss: 1.118022084236145\n",
            "Epoch: 1, Loss: 1.2480964660644531\n",
            "Epoch: 1, Loss: 1.1882997751235962\n",
            "Epoch: 1, Loss: 1.109810471534729\n",
            "Epoch: 1, Loss: 1.1602859497070312\n",
            "Epoch: 1, Loss: 1.1359280347824097\n",
            "Epoch: 1, Loss: 1.1348450183868408\n",
            "Epoch: 1, Loss: 1.1094067096710205\n",
            "Epoch: 1, Loss: 1.0774223804473877\n",
            "Epoch: 1, Loss: 1.0896989107131958\n",
            "Epoch: 1, Loss: 1.0898946523666382\n",
            "Epoch: 1, Loss: 1.110427975654602\n",
            "Epoch: 1, Loss: 1.1316590309143066\n",
            "Epoch: 1, Loss: 1.0494626760482788\n",
            "Epoch: 1, Loss: 1.061897873878479\n",
            "Epoch: 1, Loss: 1.0437201261520386\n",
            "Epoch: 1, Loss: 1.140191912651062\n",
            "Epoch: 1, Loss: 1.06553053855896\n",
            "Epoch: 1, Loss: 1.086696743965149\n",
            "Epoch: 1, Loss: 1.1567968130111694\n",
            "Epoch: 1, Loss: 1.0940122604370117\n",
            "Epoch: 1, Loss: 1.0764917135238647\n",
            "Epoch: 1, Loss: 1.0892095565795898\n",
            "Epoch: 1, Loss: 1.145736575126648\n",
            "Epoch: 1, Loss: 1.2238823175430298\n",
            "Epoch: 1, Loss: 1.141538381576538\n",
            "Epoch: 1, Loss: 1.06662118434906\n",
            "Epoch: 1, Loss: 1.071244716644287\n",
            "Epoch: 1, Loss: 1.088009238243103\n",
            "Epoch: 1, Loss: 1.1890259981155396\n",
            "Epoch: 1, Loss: 1.065535068511963\n",
            "Epoch: 1, Loss: 1.1147202253341675\n",
            "Epoch: 1, Loss: 1.2258294820785522\n",
            "Epoch: 1, Loss: 1.1515156030654907\n",
            "Epoch: 1, Loss: 1.1701489686965942\n",
            "Epoch: 1, Loss: 1.0988017320632935\n",
            "Epoch: 1, Loss: 1.110100269317627\n",
            "Epoch: 1, Loss: 1.1210153102874756\n",
            "Epoch: 1, Loss: 1.13377845287323\n",
            "Epoch: 1, Loss: 1.0570672750473022\n",
            "Epoch: 1, Loss: 1.0856074094772339\n",
            "Epoch: 1, Loss: 1.0641275644302368\n",
            "Epoch: 1, Loss: 1.1809216737747192\n",
            "Epoch: 1, Loss: 1.062122106552124\n",
            "Epoch: 1, Loss: 1.0960689783096313\n",
            "Epoch: 1, Loss: 1.1469885110855103\n",
            "Epoch: 2, Loss: 1.0880818367004395\n",
            "Epoch: 2, Loss: 1.129820704460144\n",
            "Epoch: 2, Loss: 1.119722843170166\n",
            "Epoch: 2, Loss: 1.1289405822753906\n",
            "Epoch: 2, Loss: 1.100364089012146\n",
            "Epoch: 2, Loss: 1.1202627420425415\n",
            "Epoch: 2, Loss: 1.10262930393219\n",
            "Epoch: 2, Loss: 1.1020740270614624\n",
            "Epoch: 2, Loss: 1.109592318534851\n",
            "Epoch: 2, Loss: 1.1143354177474976\n",
            "Epoch: 2, Loss: 1.0945159196853638\n",
            "Epoch: 2, Loss: 1.0728284120559692\n",
            "Epoch: 2, Loss: 1.092070460319519\n",
            "Epoch: 2, Loss: 1.124064564704895\n",
            "Epoch: 2, Loss: 1.1016740798950195\n",
            "Epoch: 2, Loss: 1.119092345237732\n",
            "Epoch: 2, Loss: 1.117146372795105\n",
            "Epoch: 2, Loss: 1.1396424770355225\n",
            "Epoch: 2, Loss: 1.0875083208084106\n",
            "Epoch: 2, Loss: 1.096956729888916\n",
            "Epoch: 2, Loss: 1.0391602516174316\n",
            "Epoch: 2, Loss: 1.1049727201461792\n",
            "Epoch: 2, Loss: 1.0801485776901245\n",
            "Epoch: 2, Loss: 1.1417279243469238\n",
            "Epoch: 2, Loss: 1.1269209384918213\n",
            "Epoch: 2, Loss: 1.0583404302597046\n",
            "Epoch: 2, Loss: 1.0657151937484741\n",
            "Epoch: 2, Loss: 1.0693894624710083\n",
            "Epoch: 2, Loss: 1.0572761297225952\n",
            "Epoch: 2, Loss: 1.214875340461731\n",
            "Epoch: 2, Loss: 1.1248396635055542\n",
            "Epoch: 2, Loss: 1.0553371906280518\n",
            "Epoch: 2, Loss: 1.084718108177185\n",
            "Epoch: 2, Loss: 0.9935600757598877\n",
            "Epoch: 2, Loss: 1.1505848169326782\n",
            "Epoch: 2, Loss: 1.1236668825149536\n",
            "Epoch: 2, Loss: 1.038063406944275\n",
            "Epoch: 2, Loss: 1.0252444744110107\n",
            "Epoch: 2, Loss: 1.110604166984558\n",
            "Epoch: 2, Loss: 1.1202336549758911\n",
            "Epoch: 2, Loss: 1.151041030883789\n",
            "Epoch: 2, Loss: 1.1216953992843628\n",
            "Epoch: 2, Loss: 1.2533618211746216\n",
            "Epoch: 2, Loss: 1.003591537475586\n",
            "Epoch: 2, Loss: 1.0854089260101318\n",
            "Epoch: 2, Loss: 1.096186876296997\n",
            "Epoch: 2, Loss: 0.9900222420692444\n",
            "Epoch: 2, Loss: 1.164443850517273\n",
            "Epoch: 2, Loss: 1.0430127382278442\n",
            "Epoch: 2, Loss: 1.2036068439483643\n",
            "Epoch: 2, Loss: 1.1171156167984009\n",
            "Epoch: 2, Loss: 1.0046344995498657\n",
            "Epoch: 2, Loss: 1.0953415632247925\n",
            "Epoch: 2, Loss: 0.9498822689056396\n",
            "Epoch: 2, Loss: 1.1954655647277832\n",
            "Epoch: 2, Loss: 1.2275837659835815\n",
            "Epoch: 2, Loss: 1.003806710243225\n",
            "Epoch: 2, Loss: 1.1866520643234253\n",
            "Epoch: 2, Loss: 1.0328272581100464\n",
            "Epoch: 2, Loss: 1.1609817743301392\n",
            "Epoch: 2, Loss: 1.0591583251953125\n",
            "Epoch: 2, Loss: 1.0095313787460327\n",
            "Epoch: 2, Loss: 1.1240900754928589\n",
            "Epoch: 2, Loss: 1.2195725440979004\n",
            "Epoch: 2, Loss: 1.1347323656082153\n",
            "Epoch: 2, Loss: 1.0804388523101807\n",
            "Epoch: 2, Loss: 1.0749350786209106\n",
            "Epoch: 2, Loss: 1.1542271375656128\n",
            "Epoch: 2, Loss: 1.1301237344741821\n",
            "Epoch: 2, Loss: 1.160403847694397\n",
            "Epoch: 2, Loss: 1.08714759349823\n",
            "Epoch: 2, Loss: 1.0926473140716553\n",
            "Epoch: 2, Loss: 1.0493602752685547\n",
            "Epoch: 2, Loss: 1.1386739015579224\n",
            "Epoch: 2, Loss: 1.0879977941513062\n",
            "Epoch: 2, Loss: 0.9965491890907288\n",
            "Epoch: 2, Loss: 1.2329055070877075\n",
            "Epoch: 2, Loss: 1.0854204893112183\n",
            "Epoch: 2, Loss: 1.1700153350830078\n",
            "Epoch: 2, Loss: 1.2124994993209839\n",
            "Epoch: 2, Loss: 1.1367512941360474\n",
            "Epoch: 2, Loss: 1.0330537557601929\n",
            "Epoch: 2, Loss: 1.1299701929092407\n",
            "Epoch: 2, Loss: 1.1139811277389526\n",
            "Epoch: 2, Loss: 1.1724622249603271\n",
            "Epoch: 2, Loss: 1.0991406440734863\n",
            "Epoch: 2, Loss: 1.1053696870803833\n",
            "Epoch: 2, Loss: 1.1092497110366821\n",
            "Epoch: 2, Loss: 1.100175142288208\n",
            "Epoch: 2, Loss: 1.083645224571228\n",
            "Epoch: 2, Loss: 1.16496741771698\n",
            "Epoch: 2, Loss: 1.133689045906067\n",
            "Epoch: 2, Loss: 1.1152600049972534\n",
            "Epoch: 2, Loss: 1.0422238111495972\n",
            "Epoch: 2, Loss: 1.1059097051620483\n",
            "Epoch: 2, Loss: 1.1054086685180664\n",
            "Epoch: 2, Loss: 1.1843518018722534\n",
            "Epoch: 2, Loss: 1.1352171897888184\n",
            "Epoch: 2, Loss: 1.1302391290664673\n",
            "Epoch: 2, Loss: 1.1688024997711182\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import torch\n",
        "import json\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AlbertTokenizer, AlbertForSequenceClassification\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, matthews_corrcoef, confusion_matrix\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Load the data\n",
        "with open('/content/casualnet.json', 'r') as f:\n",
        "    casulnet_data = json.load(f)\n",
        "\n",
        "\n",
        "random_selected_samples = random.sample(casulnet_data, 1000)\n",
        "\n",
        "# Define split sizes for an 80-20 split\n",
        "train_size = int(0.8 * len(random_selected_samples))\n",
        "train_data = random_selected_samples[:train_size]\n",
        "validation_data = random_selected_samples[train_size:]\n",
        "\n",
        "class CasulnetDataset(Dataset):\n",
        "    def __init__(self, casulnet_data, tokenizer):\n",
        "        self.casulnet_data = casulnet_data\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.casulnet_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = self.casulnet_data[idx]\n",
        "        context_question = data['context']\n",
        "        choices = [data[f'choice_id: {i}'] for i in range(3)]  # Assuming there are always 3 choices\n",
        "        label = data['label']\n",
        "\n",
        "        # Tokenizing context_question with each choice\n",
        "        input_ids = []\n",
        "        attention_masks = []\n",
        "        for choice in choices:\n",
        "            encoded_input = self.tokenizer(context_question, choice, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
        "            input_ids.append(encoded_input['input_ids'].squeeze(0))\n",
        "            attention_masks.append(encoded_input['attention_mask'].squeeze(0))\n",
        "\n",
        "        label = torch.tensor([label]*3)  # Replicate label for each choice\n",
        "        return {\n",
        "            'input_ids': torch.stack(input_ids),\n",
        "            'attention_mask': torch.stack(attention_masks),\n",
        "            'labels': label\n",
        "        }\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
        "model = AlbertForSequenceClassification.from_pretrained('albert-base-v2', num_labels=3)  # Assuming 3 possible labels\n",
        "\n",
        "casulnet_dataset = CasulnetDataset(train_data, tokenizer)\n",
        "dataloader = DataLoader(casulnet_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "for epoch in range(3):\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].view(-1, 512)  # Flatten the input for processing\n",
        "        attention_mask = batch['attention_mask'].view(-1, 512)  # Flatten the attention mask for processing\n",
        "        labels = batch['labels'].view(-1)  # Flatten the labels to match the input batch size\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
        "\n",
        "validation_dataset = CasulnetDataset(validation_data, tokenizer)\n",
        "validation_dataloader = DataLoader(validation_dataset, batch_size=64)\n",
        "\n",
        "model.eval()\n",
        "y_true, y_pred, y_scores = [], [], []\n",
        "\n",
        "def compute_metrics(y_true, y_pred, y_prob):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "    precision = precision_score(y_true, y_pred, average='weighted')\n",
        "    recall = recall_score(y_true, y_pred, average='weighted')\n",
        "    roc_auc = roc_auc_score(label_binarize(y_true, classes=[0, 1, 2]), y_prob, multi_class='ovr', average='weighted')\n",
        "    mcc = matthews_corrcoef(y_true, y_pred)\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    return accuracy, f1, precision, recall, roc_auc, mcc, conf_matrix\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in validation_dataloader:\n",
        "        input_ids = batch['input_ids'].view(-1, 512)\n",
        "        attention_mask = batch['attention_mask'].view(-1, 512)\n",
        "        labels = batch['labels'].view(-1)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs.logits, dim=1)\n",
        "        y_scores_batch = torch.softmax(outputs.logits, dim=1).cpu().numpy()\n",
        "\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(predictions.cpu().numpy())\n",
        "        y_scores.extend(y_scores_batch)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate metrics\n",
        "y_scores = np.vstack(y_scores)  # Ensure y_scores is properly shaped for multiclass ROC-AUC calculation\n",
        "accuracy, f1, precision, recall, roc_auc, mcc, conf_matrix = compute_metrics(y_true, y_pred, y_scores)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"F1: {f1:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
        "print(f\"MCC: {mcc:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQnu3W-p5_6X",
        "outputId": "be41db26-d6a5-4619-cf55-1c5fb2888702"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.3383\n",
            "F1: 0.1934\n",
            "Precision: 0.2721\n",
            "Recall: 0.3383\n",
            "ROC-AUC: 0.5125\n",
            "MCC: -0.0036\n",
            "Confusion Matrix:\n",
            "[[  2 194  11]\n",
            " [  4 196   4]\n",
            " [  3 181   5]]\n"
          ]
        }
      ]
    }
  ]
}